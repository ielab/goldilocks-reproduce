{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2189d2-42b5-4ab1-8127-b54cf5e245bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `all.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f83db4-941b-4c40-a9a8-736dd4854cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "454314it [04:10, 1815.44it/s]\n"
     ]
    }
   ],
   "source": [
    "already_got_id = set()\n",
    "\n",
    "#real_collection_file = open(collection_file, 'w')\n",
    "collection_dict = {}\n",
    "collection_file = \"./goldilocks_reproduce/collection/all.jsonl\"\n",
    "collection_lines = []\n",
    "with open(collection_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        data_list = json.loads(line)\n",
    "        id = data_list[\"pmid\"]\n",
    "        if id in already_got_id or id == \"null\" or id is None:\n",
    "            continue\n",
    "        else:\n",
    "            already_got_id.add(id)\n",
    "            title = data_list[\"title\"]\n",
    "            content = data_list[\"abstract\"]\n",
    "            new_content = title + content\n",
    "            if new_content != \"\":\n",
    "                collection_lines.append(json.dumps({id: tokenised_list}) + '\\n')\n",
    "                collection_dict[id] = tokenised_list\n",
    "#real_collection_file.writelines(collection_lines)\n",
    "#real_collection_file.close()\n",
    "collection_lines.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13775e06-3706-453f-862e-f4ef891c1c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198110"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(already_got_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0a2bcca-baaf-45cc-8039-64d9c85bfdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454273"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9171fafc-1977-47c4-9870-5f2599597322",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dict['18485144']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f719044a-6dc5-45e2-8aa8-ad63995b0797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pmid\": \"18485144\", \"title\": \"Current and future therapy in Alzheimer's disease.\", \"abstract\": \"Dementia is increasingly being recognized as one of the most important medical problems in the elderly. As most pharmacological research within the field of dementia is focused on Alzheimer's dementia (AD), this review will focus on pharmacological interventions in AD. Most disease-modifying therapies are based on the amyloid hypothesis. In this hypothesis, the pathological accumulation of Abeta in the brain leads to oxidative stress, neuronal destruction and finally the clinical syndrome of AD. Following this hypothesis, secondary prevention of AD can be made by: decreasing the production of Abeta, stimulation of clearance of Abeta formed or prevention of aggregation of Abeta into amyloid plaques. First a short overview on current approved therapies for AD is given. The main part of  the review will focus on potential disease-modifying therapies for AD that are currently being studied in phase I to phase III trials.\"}\n"
     ]
    }
   ],
   "source": [
    "!head -1 ./goldilocks_reproduce/collection/all.jsonl "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcdc285-9e40-4cab-a088-9a673b1a5261",
   "metadata": {},
   "source": [
    "#### Clean `all.jsonl` --> `all_clean.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1a5690bb-bc6b-4b9f-8768-203a7be3d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "454314it [00:03, 115421.38it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_collection_file = open(\"./goldilocks_reproduce/collection/all_clean.jsonl\", 'w')\n",
    "#clean_collection_dict = {}\n",
    "already_got_id = set()\n",
    "collection_file = \"./goldilocks_reproduce/collection/all.jsonl\"\n",
    "clean_collection_lines = []\n",
    "n_repeat = []\n",
    "n_nan = 0\n",
    "\n",
    "with open(collection_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        data_list = json.loads(line)\n",
    "        pmid = data_list[\"pmid\"]\n",
    "        if pmid in already_got_id:\n",
    "            n_repeat.append(pmid)\n",
    "            continue\n",
    "        elif pmid is None:\n",
    "            n_nan += 1\n",
    "            continue\n",
    "        else:\n",
    "            already_got_id.add(pmid)\n",
    "            title = data_list[\"title\"]\n",
    "            abstract = data_list[\"abstract\"]\n",
    "            content = title + abstract\n",
    "            if content != \"\":\n",
    "                clean_collection_lines.append(json.dumps({\"pmid\": pmid, \"title\": title, \"abstract\": abstract}) +'\\n')\n",
    "                #clean_collection_dict[pmid] = content\n",
    "clean_collection_file.writelines(clean_collection_lines)\n",
    "clean_collection_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b42af425-0439-4d0e-9d85-242750bb0c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_repeat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54764272-68db-4555-bbcb-d6fbdd5b37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11563142-d092-4c0a-a3d0-053e38d1f04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03456997871398926,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 29,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce81e8cfa9024a708bcf8a412450dbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029695987701416016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b3ed7ecc4e4b6aa5e4245950245830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03552603721618652,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 213450,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6377abe47006450e86d65a352b81215b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03484296798706055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 435797,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c91f97a5844f13b5c770c66f2c8220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "859176ef-afac-4325-8b92-187e510378ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93646bb9-142d-492f-88df-d04a6a7ae698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9668c00-d61a-48e6-b8f2-57a03027addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9fa539-ea22-46bd-9ffe-dd92d78bced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20be776-969c-41ed-a69d-a59e04c31dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b936a36-bf68-436d-8212-098fc691603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac28748d-eaed-4345-813f-d970eef6b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa0a27c7-cdc7-47d0-afa5-17ff77dcea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "415e9b02-656c-4c11-9380-f3e58dfac1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f7b6fe-0718-4062-82b7-1a28c120cf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d141afd3-0bc8-4838-982e-92d56964c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7000277-e8bc-4b18-8dfd-fe84a28bf4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb105f0-5fd7-43fc-9bfa-a84c07c75011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bad0cbd5-119c-4361-ae03-e81551e50725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82702eac-cc4e-46e1-834d-4cca4e980306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12586312-e285-435e-9dc6-951d806880af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
